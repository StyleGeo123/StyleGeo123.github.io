<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Vividportraits: Face Parsing Guided Portrait Animation">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>StyleGeoSync: High-Fidelity Lip Sync with Style-Aware Geometric Guidance of 3D Facial Segmentation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">StyleGeoSync: High-Fidelity Lip Sync with Style-Aware Geometric Guidance of 3D Facial Segmentation</h1>
            <p class="is-size-5">
              <a href="https://anonymous.4open.science/r/StyleGeoSync-CF9C" target="_blank">
                <i class="fab fa-github fa-lg"></i> Anonymous Code Repository
              </a> 
            </p>
            <div class="column has-text-centered">
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/teaser.mp4" type="video/mp4">
        </video> -->
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              For talking head generation and its downstream applications, the generalized audio-driven lip-sync technology is of great significance. However, existing methods still face challenges in eliminating artifacts in the generated frames and maintaining the personalized speaking style of the template video. In this paper, we propose StyleGeoSync, a generalized high-fidelity lip-sync framework,  consisting of style-aware audio-driven spatial geometry construction and 3D facial segmentation-guided texture generation. To achieve personalized and style-consistent lip synchronization, we introduce a style condition mechanism that disentangles lip motion into audio-related absolute dynamics and audio-independent speaking style. Moreover, to fundamentally eliminate visual artifacts, we design the texture generation as a standalone module, which prevents interference from the reference image during lip motion synthesis. By decoupling lip movements and image textures into two stages,  this framework can generate lip-synced videos with better image quality according to the given audio, and achieve specific speaking style transfer based on the reference video. Experimental results on CelebV-HQ and HDTF datasets demonstrate that our proposed method outperforms existing methods in image quality, lip synchronization, and identity preservation.
            </p>
          </div>
          <img src="./static/images/006.png">
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>




<!-- <section class="hero is-light is-small">
  <div class="hero-body">

    <h1 style="font-size: 25px; font-weight: bold;"><b>Single Portrait + Multiple Motions</b></h1></br>
    <p>第一行...</p></br>
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">

        <div class="item item-id11">
          <video preload="metadata" poster="" id="id11" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/1/video1.mp4"
                    type="video/mp4">
          </video>
        </div>

        <div class="item item-id29">
          <video preload="metadata" poster="" id="id29" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/1/video2.mp4"
                    type="video/mp4">
          </video>
        </div>
        
        <div class="item item-id30">
          <video preload="metadata" poster="" id="id30" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/1/video3.mp4"
                    type="video/mp4">
          </video>
        </div>
        
        <div class="item item-id32">
          <video preload="metadata" poster="" id="id32" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/1/video4.mp4"
                    type="video/mp4">
          </video>
        </div>
        
      </div>
    </div>
  </div>
</section> -->


<section class="hero is-light is-small">
  <div class="hero-body">
    <h1 style="font-size: 25px; font-weight: bold;"><b>Qualitative Comparison with other methods</b></h1></br>
        <div class="item item-vid0">
          <video preload="metadata" poster="" id="000" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/Qualitative_Comparison_72MB.mp4"
                    type="video/mp4">
          </video>
        </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <h1 style="font-size: 25px; font-weight: bold;"><b>Speaking in different styles</b></h1></br>
    <p>Given a template video, a driving audio, and a reference video that provides the speaking style, our method can generate high-fidelity lip-synced results with the specified speaking style under the guidance of 3D facial segmentation.</p></br>
        <div class="item item-vid0">
          <video preload="metadata" poster="" id="000" autoplay controls muted loop playsinline width="100%">
            <source src="./static/videos/Demo/video1.mp4"
                    type="video/mp4">
          </video>
        </div>
  </div>
</section>



  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
